# ============================================================================
# EXAMPLE: AMD Threadripper 1950X - Deep Parameter Sweep Mode
# ============================================================================
# This is a REAL working configuration for the Threadripper 1950X
# Use this AFTER exploratory run identifies 2-3 winning builds
#
# Deep mode = parameter exploration (KV cache, MLA, batch sizes)
# NOT just "run winners with more reps"
#
# 16 physical cores (0-15), 2 NUMA nodes, SMT enabled
# NUMA node 0: cores 0-7
# NUMA node 1: cores 8-15
# ============================================================================

mode: deep
repetitions: 3  # Breadth over depth

model_path: /path/to/your/model.gguf
model_info: "llama-3.1-8B-Q4_K_M"

# Builds from exploratory winners (top 2-3)
builds:
  ik_vanilla:
    binary: /home/nikro/projects/ik_llama.cpp/build-cpu/bin/llama-bench
    label: "IK-vanilla (winner)"
    
  ik_fancy:
    binary: /home/nikro/projects/ik_llama.cpp/build-fancy/bin/llama-bench
    label: "IK-fancy"

builds_select:
  - ik_vanilla
  - ik_fancy

# Test the winning NUMA config from exploratory
test_matrix:
  - name: "optimal"
    numactl: "-N 0,1 -m 0,1 --physcpubind=0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15"
    env:
      OMP_NUM_THREADS: "16"
    extra_args: "-t 16"

# Same metrics as exploratory
metrics:
  - pp512
  - tg128
  - mixed

output_dir: ./reports

# ============================================================================
# PARAMETER SWEEP (this is what makes "deep" mode deep!)
# ============================================================================

parameter_sweep:
  
  # KV Cache Type Variations
  kv_cache:
    - name: "f16_f16"
      args: "-ctk f16 -ctv f16"  # Baseline
    
    - name: "f8_f16"
      args: "-ctk f8 -ctv f16"   # Quantize K cache (common optimization)
    
    - name: "f16_f8"
      args: "-ctk f16 -ctv f8"   # Quantize V cache
  
  # MLA/Attention Variants
  mla_variants:
    - name: "baseline"
      args: ""
    
    - name: "mla2_fa_fmoe"
      args: "-mla 2 -fa -fmoe"
    
    - name: "mla3_fa_fmoe"
      args: "-mla 3 -fa -fmoe"
    
    - name: "mla2_fa"
      args: "-mla 2 -fa"  # Without fused MoE
  
  # Batch Size Variations
  batch_sizes:
    - name: "std"
      args: "-b 2048 -ub 512"
    
    - name: "small_128"
      args: "-b 256 -ub 128"
    
    - name: "small_64"
      args: "-b 256 -ub 64"
    
    - name: "mid"
      args: "-b 512 -ub 256"

# ============================================================================
# MATRIX SIZE: 2 builds × 1 NUMA × 3 metrics × 3 KV × 4 MLA × 4 batch
#              = 288 tests × 3 reps = 864 runs (~6-8 hours)
# ============================================================================
